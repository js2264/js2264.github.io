<!DOCTYPE html>
<html lang="en-US">
<head>

<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta http-equiv="content-type" content="text/html; charset=utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<meta name="description" content="JS">
<meta name="keywords" content="developer,coding,genomics,blog,hugo">

<base href="/">

<title>Jacques Serizay</title>

<meta name="generator" content="Hugo 0.91.2" />





<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lato:300,400|Roboto+Slab:400,700|Roboto:300,300i,400,400i,500,500i,700,700i">


<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css">
<link rel="stylesheet" href="/css/main.css">
<link rel="stylesheet" href="/css/custom.css">




<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
<link rel="icon" type="image/png" href="/favicon-32x32.png" sizes="32x32">
<link rel="icon" type="image/png" href="/favicon-16x16.png" sizes="16x16">
<link rel="manifest" href="/manifest.json">
<link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">
<meta name="theme-color" content="#ffffff">

</head>
<body lang="en-US">
<div class="container">


<header class="row text-left title">
  <h1 class="title">Machine Learning and Natural Langage Processing</h1>
</header>
<section id="category-pane" class="row meta">
  
  <div class="col-md-12">
    <h6 class="text-left meta">
      PUBLISHED ON JUN 13, 2020
      
      
      
      â€”
      
      
      <a class="meta" href="/categories/nlp">NLP</a>, 
      
      <a class="meta" href="/categories/r">R</a>, 
      
      <a class="meta" href="/categories/text-mining">TEXT MINING</a>
      
      
      
    </h6>
  </div>
  
</section>
<section id="content-pane" class="row">
  <div class="col-md-12 text-justify content">
    
     <img src="img/blog/there_once_was_a_hobbit.png" alt="Post Image" style="max-width:100%;max-height:100%;"> 
    
</div>
  <div class="col-md-12 text-justify content">
    
<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>
<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  background-color: #f8f8f8; }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ef2929; } /* Alert */
code span.an { color: #8f5902; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #c4a000; } /* Attribute */
code span.bn { color: #0000cf; } /* BaseN */
code span.cf { color: #204a87; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4e9a06; } /* Char */
code span.cn { color: #000000; } /* Constant */
code span.co { color: #8f5902; font-style: italic; } /* Comment */
code span.cv { color: #8f5902; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #8f5902; font-weight: bold; font-style: italic; } /* Documentation */
code span.dt { color: #204a87; } /* DataType */
code span.dv { color: #0000cf; } /* DecVal */
code span.er { color: #a40000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #0000cf; } /* Float */
code span.fu { color: #000000; } /* Function */
code span.im { } /* Import */
code span.in { color: #8f5902; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #204a87; font-weight: bold; } /* Keyword */
code span.op { color: #ce5c00; font-weight: bold; } /* Operator */
code span.ot { color: #8f5902; } /* Other */
code span.pp { color: #8f5902; font-style: italic; } /* Preprocessor */
code span.sc { color: #000000; } /* SpecialChar */
code span.ss { color: #4e9a06; } /* SpecialString */
code span.st { color: #4e9a06; } /* String */
code span.va { color: #000000; } /* Variable */
code span.vs { color: #4e9a06; } /* VerbatimString */
code span.wa { color: #8f5902; font-weight: bold; font-style: italic; } /* Warning */
</style>


<p>Inspired from <a href="https://machinelearningmastery.com/how-to-develop-a-word-level-neural-language-model-in-keras/" class="uri">https://machinelearningmastery.com/how-to-develop-a-word-level-neural-language-model-in-keras/</a></p>
<div id="clean-up-data-and-select-results-chapters" class="section level2">
<h2>Clean-up data and select Results chapters</h2>
<div class="sourceCode" id="cb1"><pre class="sourceCode sh"><code class="sourceCode bash"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true"></a><span class="fu">cat</span> txt/collated_books.txt <span class="kw">|</span> <span class="kw">\</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true"></a>    <span class="fu">tr</span> <span class="st">&#39;\n&#39;</span> <span class="st">&#39; &#39;</span> <span class="kw">|</span> <span class="kw">\</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true"></a>    <span class="fu">sed</span> <span class="st">&quot;s,&#39;, ,g&quot;</span> <span class="kw">|</span> <span class="kw">\</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true"></a>    <span class="fu">sed</span> <span class="st">&quot;s,_,,g&quot;</span> <span class="kw">|</span> <span class="kw">\</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true"></a>    <span class="fu">sed</span> <span class="st">&#39;s,[ ]\+, ,g&#39;</span> <span class="kw">|</span> <span class="kw">\</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true"></a>    <span class="fu">sed</span> <span class="st">&quot;s,^ ,,g&quot;</span> <span class="op">&gt;</span> txt/collated_books_stripped.txt</span></code></pre></div>
</div>
<div id="create-a-new-conda-env." class="section level2">
<h2>Create a new conda env.</h2>
<div class="sourceCode" id="cb2"><pre class="sourceCode sh"><code class="sourceCode bash"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true"></a><span class="ex">conda</span> create -n keras python=3.6 tensorflow keras nltk</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true"></a><span class="ex">conda</span> activate keras</span></code></pre></div>
</div>
<div id="start-python" class="section level2">
<h2>Start python</h2>
<div class="sourceCode" id="cb3"><pre class="sourceCode py"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true"></a><span class="im">import</span> tensorflow <span class="im">as</span> tf</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true"></a>tf.config.threading.set_inter_op_parallelism_threads(<span class="dv">1</span>)</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true"></a>tf.config.threading.set_intra_op_parallelism_threads(<span class="dv">1</span>)</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true"></a><span class="im">import</span> keras</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true"></a><span class="im">from</span> pickle <span class="im">import</span> dump</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true"></a><span class="im">from</span> keras <span class="im">import</span> backend <span class="im">as</span> K</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true"></a><span class="im">from</span> keras.preprocessing.text <span class="im">import</span> Tokenizer</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true"></a><span class="im">from</span> keras.utils <span class="im">import</span> to_categorical</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true"></a><span class="im">from</span> keras.models <span class="im">import</span> Sequential</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true"></a><span class="im">from</span> keras.layers <span class="im">import</span> Dense</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true"></a><span class="im">from</span> keras.layers <span class="im">import</span> LSTM</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true"></a><span class="im">from</span> keras.layers <span class="im">import</span> Dropout</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true"></a><span class="im">from</span> keras.layers <span class="im">import</span> Embedding</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true"></a><span class="im">from</span> nltk.tokenize <span class="im">import</span> RegexpTokenizer</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true"></a><span class="im">from</span> nltk.tokenize <span class="im">import</span> WordPunctTokenizer</span></code></pre></div>
</div>
<div id="import-and-clean-up-data" class="section level2">
<h2>Import and clean-up data</h2>
<div class="sourceCode" id="cb4"><pre class="sourceCode py"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true"></a><span class="bu">file</span> <span class="op">=</span> <span class="bu">open</span>(<span class="st">&#39;txt/collated_books_stripped.txt&#39;</span>, <span class="st">&#39;r&#39;</span>)</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true"></a>doc <span class="op">=</span> <span class="bu">file</span>.read().lower()</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true"></a><span class="bu">file</span>.close()</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true"></a>tokenizer <span class="op">=</span> RegexpTokenizer(<span class="st">&#39;\w+|\.|,&#39;</span>)</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true"></a>tokens <span class="op">=</span> tokenizer.tokenize(doc)</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true"></a><span class="bu">print</span>(tokens[:<span class="dv">20</span>])</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true"></a><span class="bu">print</span>(<span class="st">&#39;Total Tokens: </span><span class="sc">%d</span><span class="st">&#39;</span> <span class="op">%</span> <span class="bu">len</span>(tokens))</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true"></a><span class="bu">print</span>(<span class="st">&#39;Unique Tokens: </span><span class="sc">%d</span><span class="st">&#39;</span> <span class="op">%</span> <span class="bu">len</span>(<span class="bu">set</span>(tokens)))</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true"></a><span class="co"># organize into sequences of tokens</span></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true"></a>length <span class="op">=</span> <span class="dv">50</span> <span class="op">+</span> <span class="dv">1</span></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true"></a>sequences <span class="op">=</span> <span class="bu">list</span>()</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(length, <span class="bu">len</span>(tokens)):</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true"></a>    seq <span class="op">=</span> tokens[i<span class="op">-</span>length:i]</span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true"></a>    line <span class="op">=</span> <span class="st">&#39; &#39;</span>.join(seq)</span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true"></a>    sequences.append(line)</span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true"></a></span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true"></a>data <span class="op">=</span> <span class="st">&#39;</span><span class="ch">\n</span><span class="st">&#39;</span>.join(sequences)</span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true"></a><span class="bu">file</span> <span class="op">=</span> <span class="bu">open</span>(<span class="st">&#39;txt/collated_books_stripped_sequences.txt&#39;</span>, <span class="st">&#39;w&#39;</span>)</span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true"></a><span class="bu">file</span>.write(data)</span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true"></a><span class="bu">file</span>.close()</span></code></pre></div>
</div>
<div id="start-working-with-the-data" class="section level2">
<h2>Start working with the data</h2>
<div class="sourceCode" id="cb5"><pre class="sourceCode py"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true"></a><span class="co"># Import the text</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true"></a><span class="bu">file</span> <span class="op">=</span> <span class="bu">open</span>(<span class="st">&#39;txt/collated_books_stripped_sequences.txt&#39;</span>, <span class="st">&#39;r&#39;</span>)</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true"></a>doc <span class="op">=</span> <span class="bu">file</span>.read()</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true"></a><span class="bu">file</span>.close()</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true"></a>lines <span class="op">=</span> doc.split(<span class="st">&#39;</span><span class="ch">\n</span><span class="st">&#39;</span>)</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true"></a></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true"></a><span class="co"># Tokenize</span></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true"></a>tokenizer <span class="op">=</span> Tokenizer(filters <span class="op">=</span> <span class="st">&#39;!&quot;#$%&amp;()*+-/:;&lt;=&gt;?@[\]^_`{|}~</span><span class="ch">\t\n</span><span class="st">&#39;</span>)</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true"></a>tokenizer.fit_on_texts(lines)</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true"></a>sequences <span class="op">=</span> tokenizer.texts_to_sequences(lines)</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true"></a>vocab_size <span class="op">=</span> <span class="bu">len</span>(tokenizer.word_index) <span class="op">+</span> <span class="dv">1</span></span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true"></a></span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true"></a><span class="co"># Hot one encode</span></span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true"></a>sequences <span class="op">=</span> np.array(sequences)</span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true"></a>X, y <span class="op">=</span> sequences[:,:<span class="op">-</span><span class="dv">1</span>], sequences[:,<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true"></a>y <span class="op">=</span> keras.utils.to_categorical(y, num_classes <span class="op">=</span> vocab_size)</span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true"></a>seq_length <span class="op">=</span> X.shape[<span class="dv">1</span>]</span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true"></a></span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true"></a><span class="co"># define model</span></span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true"></a>model <span class="op">=</span> Sequential()</span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true"></a>model.add(Embedding(vocab_size, seq_length, input_length <span class="op">=</span> seq_length))</span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true"></a>model.add(LSTM(<span class="dv">30</span>))</span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true"></a>model.add(Embedding(<span class="dv">30</span>, <span class="dv">100</span>))</span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true"></a>model.add(Dropout(<span class="fl">0.15</span>))</span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true"></a>model.add(Dense(vocab_size, activation<span class="op">=</span><span class="st">&#39;softmax&#39;</span>))</span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true"></a><span class="bu">print</span>(model.summary())</span>
<span id="cb5-27"><a href="#cb5-27" aria-hidden="true"></a></span>
<span id="cb5-28"><a href="#cb5-28" aria-hidden="true"></a><span class="co"># compile model</span></span>
<span id="cb5-29"><a href="#cb5-29" aria-hidden="true"></a>model.<span class="bu">compile</span>(loss<span class="op">=</span><span class="st">&#39;categorical_crossentropy&#39;</span>, optimizer<span class="op">=</span><span class="st">&#39;adam&#39;</span>, metrics<span class="op">=</span>[<span class="st">&#39;accuracy&#39;</span>])</span>
<span id="cb5-30"><a href="#cb5-30" aria-hidden="true"></a></span>
<span id="cb5-31"><a href="#cb5-31" aria-hidden="true"></a><span class="co"># fit model</span></span>
<span id="cb5-32"><a href="#cb5-32" aria-hidden="true"></a>history <span class="op">=</span> model.fit(X, y, batch_size <span class="op">=</span> <span class="dv">256</span>, epochs <span class="op">=</span> <span class="dv">10</span>)</span>
<span id="cb5-33"><a href="#cb5-33" aria-hidden="true"></a></span>
<span id="cb5-34"><a href="#cb5-34" aria-hidden="true"></a><span class="co"># save the model to file</span></span>
<span id="cb5-35"><a href="#cb5-35" aria-hidden="true"></a>model.save(<span class="st">&#39;model.h5&#39;</span>)</span>
<span id="cb5-36"><a href="#cb5-36" aria-hidden="true"></a></span>
<span id="cb5-37"><a href="#cb5-37" aria-hidden="true"></a><span class="co"># save the tokenizer</span></span>
<span id="cb5-38"><a href="#cb5-38" aria-hidden="true"></a>dump(tokenizer, <span class="bu">open</span>(<span class="st">&#39;tokenizer.pkl&#39;</span>, <span class="st">&#39;wb&#39;</span>))</span></code></pre></div>
</div>
<div id="predict-text" class="section level2">
<h2>Predict text</h2>
<div class="sourceCode" id="cb6"><pre class="sourceCode py"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true"></a><span class="im">from</span> random <span class="im">import</span> randint</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true"></a><span class="im">from</span> pickle <span class="im">import</span> load</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true"></a><span class="im">from</span> keras.models <span class="im">import</span> load_model</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true"></a><span class="im">from</span> keras.preprocessing.sequence <span class="im">import</span> pad_sequences</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true"></a><span class="co"># generate a sequence from a language model</span></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true"></a><span class="kw">def</span> generate_seq(model, tokenizer, seq_length, seed_text, n_words):</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true"></a>    result <span class="op">=</span> <span class="bu">list</span>()</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true"></a>    in_text <span class="op">=</span> seed_text</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true"></a>    <span class="co"># generate a fixed number of words</span></span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(n_words):</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true"></a>        <span class="co"># encode the text as integer</span></span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true"></a>        encoded <span class="op">=</span> tokenizer.texts_to_sequences([in_text])[<span class="dv">0</span>]</span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true"></a>        <span class="co"># truncate sequences to a fixed length</span></span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true"></a>        encoded <span class="op">=</span> pad_sequences([encoded], maxlen<span class="op">=</span>seq_length, truncating<span class="op">=</span><span class="st">&#39;pre&#39;</span>)</span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true"></a>        <span class="co"># predict probabilities for each word</span></span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true"></a>        yhat <span class="op">=</span> model.predict_classes(encoded, verbose<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true"></a>        <span class="co"># map predicted word index to word</span></span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true"></a>        out_word <span class="op">=</span> <span class="st">&#39;&#39;</span></span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true"></a>        <span class="cf">for</span> word, index <span class="kw">in</span> tokenizer.word_index.items():</span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true"></a>            <span class="cf">if</span> index <span class="op">==</span> yhat:</span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true"></a>                out_word <span class="op">=</span> word</span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true"></a>                <span class="cf">break</span></span>
<span id="cb6-23"><a href="#cb6-23" aria-hidden="true"></a>        <span class="co"># append to input</span></span>
<span id="cb6-24"><a href="#cb6-24" aria-hidden="true"></a>        in_text <span class="op">+=</span> <span class="st">&#39; &#39;</span> <span class="op">+</span> out_word</span>
<span id="cb6-25"><a href="#cb6-25" aria-hidden="true"></a>        result.append(out_word)</span>
<span id="cb6-26"><a href="#cb6-26" aria-hidden="true"></a>    <span class="cf">return</span> <span class="st">&#39; &#39;</span>.join(result)</span>
<span id="cb6-27"><a href="#cb6-27" aria-hidden="true"></a></span>
<span id="cb6-28"><a href="#cb6-28" aria-hidden="true"></a><span class="co"># load the model</span></span>
<span id="cb6-29"><a href="#cb6-29" aria-hidden="true"></a>model <span class="op">=</span> load_model(<span class="st">&#39;model.h5&#39;</span>)</span>
<span id="cb6-30"><a href="#cb6-30" aria-hidden="true"></a></span>
<span id="cb6-31"><a href="#cb6-31" aria-hidden="true"></a><span class="co"># load the tokenizer</span></span>
<span id="cb6-32"><a href="#cb6-32" aria-hidden="true"></a>tokenizer <span class="op">=</span> load(<span class="bu">open</span>(<span class="st">&#39;tokenizer.pkl&#39;</span>, <span class="st">&#39;rb&#39;</span>))</span>
<span id="cb6-33"><a href="#cb6-33" aria-hidden="true"></a></span>
<span id="cb6-34"><a href="#cb6-34" aria-hidden="true"></a><span class="co"># generate new text</span></span>
<span id="cb6-35"><a href="#cb6-35" aria-hidden="true"></a>seed_text <span class="op">=</span> <span class="st">&#39;ever since his return , frodo did not know this when gandalf arrived . But the mountains were close and pippin could not abandon merry there &#39;</span></span>
<span id="cb6-36"><a href="#cb6-36" aria-hidden="true"></a>generated <span class="op">=</span> generate_seq(model, tokenizer, <span class="dv">100</span>, seed_text, <span class="dv">100</span>)</span>
<span id="cb6-37"><a href="#cb6-37" aria-hidden="true"></a>seed_text <span class="op">+</span> <span class="st">&#39;///&#39;</span> <span class="op">+</span> generated</span></code></pre></div>
</div>

  </div>
</section>
<section id="tag-pane" class="row meta">
  
  <div class="col-md-12">
    <h6 class="text-right meta">
      
      
      
      TAGS:
      
      
      <a class="meta" href="/tags/nlp">NLP</a>, 
      
      <a class="meta" href="/tags/r">R</a>, 
      
      <a class="meta" href="/tags/text-mining">TEXT MINING</a>
      
      
      
    </h6>
  </div>
  
</section>




<section id="menu-pane" class="row menu text-center">
  
  
  <span><a class="menu-item" href="/blog/awesome_bioconductor/">&lt; prev | </a></span>
  
  
  <span><a class="menu-item" href="/blog">blog</a></span>
  
  
  <span><a class="menu-item" href="/blog/gganimate/"> | next &gt;</a></span>
  
  
  <h4 class="text-center"><a class="menu-item" href="/">home</a></h4>
</section>



<footer class="row text-center footer">
  <hr />
  
  <h6 class="text-center copyright">Â© 2021. Jacques Serizay. <a href="http://creativecommons.org/licenses/by/3.0/">Some Rights Reserved</a>.</h6>
  
</footer>

</div>






<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'XYZ', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>

<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.12.4/jquery.min.js"></script>
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script>
<script src="js/main.js"></script>

</body>
</html>


